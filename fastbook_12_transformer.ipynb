{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fastbook_12_transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arampacha/fastai_nbs/blob/main/fastbook_12_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1vMAp9RdEME",
        "outputId": "a7bb76b7-cb5d-46ff-ce11-1fa0aa19530b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -Uqq fastai"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 194kB 6.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 5.9MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iG8IqzSYhst"
      },
      "source": [
        "## A Transformer based Language Model form scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9xmJtyEYhsv"
      },
      "source": [
        "In this notebook i'm going to construct transformer based language model from scratch starting with the simplest building blocks. This is inspired by Chapter 12 of [Deep Learning for Coders book](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527) in which it's demonstrated how to create a Recurrent Neural Network. It provides a strong intuition of how RNNs relate to regular feed-forward neural nets and why certain design choices were made. Here we aim to aquire similar kind of intuition about Transfomer based architectures.\n",
        "\n",
        "But as always we should start with the data to be modeled, 'cause without data any model makes no particular sense."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY6kxH-MYhsz"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NDCvszHYhs1"
      },
      "source": [
        "Similar to authors of the book I'll use simple Human numbers dataset which is specifically designed to prototyping model fast and straightforward. For more details on the data one can refer to the aforemantioned book chapter which is also available for free as [a notebook](https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb) (isn't that awesome?!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVbdeXJLfDWA",
        "outputId": "1e91ecbb-9451-4ced-cea4-5fe86407befe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "from fastai.text.all import *\n",
        "path = untar_data(URLs.HUMAN_NUMBERS)\n",
        "Path.BASE_PATH = path\n",
        "path.ls()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#2) [Path('valid.txt'),Path('train.txt')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGkXUSQYYhs_"
      },
      "source": [
        "The data consists of consecutive numbers from 1 to 9999 inclusive spelled as words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12L4Y2IZfP3U",
        "outputId": "fea0eb92-f883-4455-859d-322ea8554f0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lines = L()\n",
        "with open(path/'train.txt') as f: lines += L(*f.readlines())\n",
        "with open(path/'valid.txt') as f: lines += L(*f.readlines())\n",
        "lines"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#9998) ['one \\n','two \\n','three \\n','four \\n','five \\n','six \\n','seven \\n','eight \\n','nine \\n','ten \\n'...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DmYFw56fVkG",
        "outputId": "54ecee0e-3bcf-4d73-a703-39df3fae5ef3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "text = ' . '.join([l.strip() for l in lines])\n",
        "tokens = text.split(' ')\n",
        "tokens[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['one', '.', 'two', '.', 'three', '.', 'four', '.', 'five', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShoK-h1tfd7v",
        "outputId": "693074d8-8d3f-40d1-f332-906f5b05a226",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vocab = L(*tokens).unique()\n",
        "vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#30) ['one','.','two','three','four','five','six','seven','eight','nine'...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEJDCek3fi1Q",
        "outputId": "ed89d572-0053-48c8-91d7-7e09b93ce126",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word2idx = {w:i for i,w in enumerate(vocab)}\n",
        "nums = L(word2idx[i] for i in tokens)\n",
        "nums"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#63095) [0,1,2,1,3,1,4,1,5,1...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n6KUlbMYhtd"
      },
      "source": [
        "The task will be to predict subsequent token given preceding three. This kind of tasks when the goal is to predict next token from previous ones is called autoregresive language modeling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvtW2XIVfqa_",
        "outputId": "1af41d60-58c7-4ff4-992b-2124bd3ab1af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "L((tokens[i:i+3], tokens[i+3]) for i in range(0,len(tokens)-4,3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#21031) [(['one', '.', 'two'], '.'),(['.', 'three', '.'], 'four'),(['four', '.', 'five'], '.'),(['.', 'six', '.'], 'seven'),(['seven', '.', 'eight'], '.'),(['.', 'nine', '.'], 'ten'),(['ten', '.', 'eleven'], '.'),(['.', 'twelve', '.'], 'thirteen'),(['thirteen', '.', 'fourteen'], '.'),(['.', 'fifteen', '.'], 'sixteen')...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLkeIFFafwyb",
        "outputId": "2b0f6ab2-0998-4f66-faba-2601c692db99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3))\n",
        "seqs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10,  1, 11]), 1),(tensor([ 1, 12,  1]), 13),(tensor([13,  1, 14]), 1),(tensor([ 1, 15,  1]), 16)...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiZ42KiSf4xZ"
      },
      "source": [
        "bs = 64\n",
        "cut = int(len(seqs) * 0.8)\n",
        "dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gR7uCM3fYhtv",
        "outputId": "c7a69b92-64a8-4b84-9841-c594e302b7e7"
      },
      "source": [
        "x, y = dls.one_batch()\n",
        "x.shape, y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 3]), torch.Size([64]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "Ey8OWvHTtdPQ"
      },
      "source": [
        "### Dot product attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "LcFJaSnwgfuz"
      },
      "source": [
        "![Multi head attention](https://github.com/fastai/course-v3/blob/master/nbs/dl2/images/attention.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "rr3xoXP1Yht5"
      },
      "source": [
        "The core idea behind Transformers is Attention. It was introduced in . Since the release of famous paper Attention is All You Need transformers has become most popular architecture for language modelling. \n",
        "\n",
        "There are a lot of great resourses explaining transformers architecture. I'll list some of those I found useful and comprehensive:\n",
        "1. [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) completes the original paper with code\n",
        "2. [Encoder-Decoder Model](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Encoder_Decoder_Model.ipynb) notebook by huggingface gives mathemetically grounded explanation of how transformer encoder-decoder models work\n",
        "3. [The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/) one of the great blogposts by Jay Alammar visualizing generative language modelling on exaple of GPT-2\n",
        "4. [minGPT](https://github.com/karpathy/minGPT) cool repo by A. Karpathy providing clear minimal implementation of GPT model\n",
        "\n",
        "There exist multiple attention mechanisms. The particular one used in the original transformer paper is Scaled Dot Product attention.\n",
        "Given query vector for particular token we will compare it with a key vector for each token in a sequence and decide how much value vectors of those will effect resulting representetion of the token of interest. One way to view this from a linguistic prospective is: a key is a question each word respondes to, value is information that word represent and a query is related to what every word was looking to combine with.\n",
        "\n",
        "Mathemetically we can compute attention for all _q_, _k_, _v_ in a matrix form:\n",
        "\n",
        "$$\\textbf {Attention}(Q,K,V) = \\textbf {softmax}({QK^T\\over\\sqrt d_k})V $$\n",
        "\n",
        "Note that dot product $QK^T$ results in matrix of shape (seq_len x seq_len). Then it is devided by $ \\sqrt d_k$ to compensate the fact, that longer sequences will have larger dot product. $ \\textbf{softmax}$ is applied to rescale the attention matrix to be betwin 0 and 1. When multiplied by $V$ it produces a matrix of the same shape as $V$ (seq_len x dv).\n",
        "\n",
        "So where those _q_, _k_, _v_ come from. Well that's fairly straitforward queries are culculated from the embeddings of tokens we want to find representation for. Keys and values are calculated from the embeddings of context tokens. In case of self attention all of them come from the original sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "5zoekAIjggu0"
      },
      "source": [
        "class SelfAttention(Module):\n",
        "    def __init__(self, d_in, d_qk, d_v=None):\n",
        "        d_v = ifnone(d_v, d_qk)\n",
        "        self.iq = nn.Linear(d_in, d_qk)\n",
        "        self.ik = nn.Linear(d_in, d_qk)\n",
        "        self.iv = nn.Linear(d_in, d_v)\n",
        "        self.out = nn.Linear(d_v, d_in)\n",
        "        self.scale = d_qk**0.5\n",
        "    def forward(self, x):\n",
        "        q, k, v = self.iq(x), self.ik(x), self.iv(x)\n",
        "        return self.out(F.softmax(q@k.transpose(-2,-1)/self.scale, -1)@v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "y0BLuZcSYhuB"
      },
      "source": [
        "Even though self attention mechanism is extremely useful it posseses limited expressive power. Essentially we are computing weighted some of the input modified by single affine transformation, shared across the whole sequence. To add more computational power to the model we can introduce fully connected feedforward network on top of the SelfAttention layer.\n",
        "\n",
        "Curious reader can find detailed formal analysis of the roles of SelfAttention and FeedForward layers in transformer architecture in [this paper](https://arxiv.org/pdf/1912.10077.pdf) by C. Yun et al.\n",
        "In brief the authors state that SelfAttention layers compute precise contextual maps and FeedForward layers then assign the results of these contextual maps to the desired output values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "a6f2StdmsSG9"
      },
      "source": [
        "class FeedForward(Module):\n",
        "    def __init__(self, d_in, d_ff):\n",
        "        self.lin1 = nn.Linear(d_in, d_ff)\n",
        "        self.lin2 = nn.Linear(d_ff, d_in)\n",
        "        self.act = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.lin2(self.act(self.lin1(x)))\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "G99sPt8tYhuG"
      },
      "source": [
        "The output would be of shape (bs, seq_len, d) which then may be mapped to (bs, seq_len, vocab_sz) using linear layer. But we have only one target. To adress this issue we can simply do average pooling over seq_len dimention.\n",
        "\n",
        "The resulting model is fairly simple:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "7ENQSP5HYhuI"
      },
      "source": [
        "class Model1(Module):\n",
        "    def __init__(self, vocab_sz, d_model, d_qk, d_ff):\n",
        "        self.emb = Embedding(vocab_sz, d_model)\n",
        "        self.attn = SelfAttention(d_model, d_qk)\n",
        "        self.ff = FeedForward(d_model, d_ff)\n",
        "        self.out = nn.Linear(d_model, vocab_sz)\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        x = self.ff(self.attn(x))\n",
        "        x = x.mean(1)\n",
        "        return self.out(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "JPUJTnaoYhuQ",
        "outputId": "c1f7c294-b6b4-4f63-bd06-f46b6120c025"
      },
      "source": [
        "model = Model1(len(vocab), 64, 64, 128)\n",
        "out = model(x)\n",
        "out.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 30])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "GvyTjPl_YhuW",
        "outputId": "63be6048-0f6b-4284-aff6-a4ce87c6eb00"
      },
      "source": [
        "learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n",
        "learn.lr_find()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SuggestedLRs(lr_min=0.004786301031708717, lr_steep=0.019054606556892395)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "w6seK1Y8Yhua",
        "outputId": "3f6e90a6-0dfa-4aee-909a-247731312a75"
      },
      "source": [
        "learn.fit_one_cycle(5, 1e-2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.849885</td>\n",
              "      <td>2.357760</td>\n",
              "      <td>0.423342</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.610287</td>\n",
              "      <td>2.051206</td>\n",
              "      <td>0.455907</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.992329</td>\n",
              "      <td>2.104619</td>\n",
              "      <td>0.423342</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.750790</td>\n",
              "      <td>1.790652</td>\n",
              "      <td>0.445686</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.691531</td>\n",
              "      <td>1.759753</td>\n",
              "      <td>0.467792</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "2S3m5NRWYhuf"
      },
      "source": [
        "To evaluete the model performance we need to compare it to some baseline. Let's see what would be the accuracy if of the model which would always predict most common token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "pWqBswuFYhuh",
        "outputId": "dd3bb207-4a33-4cd7-9936-f220946b97ad"
      },
      "source": [
        "n,counts = 0,torch.zeros(len(vocab))\n",
        "for x,y in dls.valid:\n",
        "    n += y.shape[0]\n",
        "    for i in range_of(vocab): counts[i] += (y==i).long().sum()\n",
        "idx = torch.argmax(counts)\n",
        "idx, vocab[idx.item()], counts[idx].item()/n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(29), 'thousand', 0.15165200855716662)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "JgsU1NY-Yhul"
      },
      "source": [
        "As you can see, always predicting \"thousand\" which turn out to be the most common token in the dataset would result in ~15% accuracy. Our simple transformer does much better then that. It feels promising, so let's try to improve the architecture and check if we can get better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "T5ZKRQu2Yhum"
      },
      "source": [
        "### Multihead attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "-mCN9cmpYhun"
      },
      "source": [
        "A structured sequence may comprise multiple distinctive kinds of relationships. Our model is forced to learn only one way in which queries, keys and values are constructed from the original token embedding. To remove this limitation we can modify attention layer include multiple heads which would correspond to extracting different kinds of relationships between tokens. The MultiHeadAttention layer consits of several heads each of those is similar to SelfAttention layer we made before. To keep computational cost of the multi-head layer we set $d_k = d_v = d_{model}/n_h$, where $n_h$ is number of heads."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "Fulk4idYYhun"
      },
      "source": [
        "class SelfAttention(Module):\n",
        "    def __init__(self, d_in, d_qk, d_v=None):\n",
        "        d_v = ifnone(d_v, d_qk)\n",
        "        self.iq = nn.Linear(d_in, d_qk)\n",
        "        self.ik = nn.Linear(d_in, d_qk)\n",
        "        self.iv = nn.Linear(d_in, d_v)\n",
        "        self.scale = d_qk**0.5\n",
        "    def forward(self, x):\n",
        "        q, k, v = self.iq(x), self.ik(x), self.iv(x)\n",
        "        return F.softmax(q@k.transpose(-2,-1)/self.scale, -1)@v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "JesAIFH-Yhut"
      },
      "source": [
        "class MultiHeadAttention(Module):\n",
        "    def __init__(self, d_model, n_heads, d_qk=None, d_v=None):\n",
        "        d_qk = ifnone(d_qk, d_model//n_heads)\n",
        "        d_v = ifnone(d_v, d_qk)\n",
        "        self.heads = nn.ModuleList([SelfAttention(d_model, d_qk) for _ in range(n_heads)])\n",
        "        self.out = nn.Linear(d_v*n_heads, d_model)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = [m(x) for m in self.heads]\n",
        "        return self.out(torch.cat(out, -1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "9Ykre2NtYhu0",
        "outputId": "bda4cf98-e877-420a-f653-c1f0899c13ac"
      },
      "source": [
        "inp = torch.randn(8, 10, 64)\n",
        "mha = MultiHeadAttention(64, 8)\n",
        "out = mha(inp)\n",
        "out.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 10, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "GwEM6yTHYhu4"
      },
      "source": [
        "class Model2(Module):\n",
        "    def __init__(self, vocab_sz, d_model=64, n_heads=4, d_ff=64*4):\n",
        "        self.emb = nn.Embedding(vocab_sz, d_model)\n",
        "        self.attn = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ff = FeedForward(d_model, d_ff)\n",
        "        self.out = nn.Linear(d_model, vocab_sz)\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        x = self.ff(self.attn(x))\n",
        "        x = x.mean(1)\n",
        "        return self.out(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "ByF7ib1OYhu9",
        "outputId": "1d4bcf91-e56e-4f3b-e69a-9d2d480e781a"
      },
      "source": [
        "learn = Learner(dls, Model2(len(vocab)), loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n",
        "learn.fit_one_cycle(5, 1e-3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.777616</td>\n",
              "      <td>2.039720</td>\n",
              "      <td>0.417400</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.538940</td>\n",
              "      <td>1.959126</td>\n",
              "      <td>0.456382</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.568550</td>\n",
              "      <td>1.849972</td>\n",
              "      <td>0.461612</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.582738</td>\n",
              "      <td>1.678933</td>\n",
              "      <td>0.468743</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.573467</td>\n",
              "      <td>1.682071</td>\n",
              "      <td>0.468743</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "7CMwwSTEYhvC"
      },
      "source": [
        "### MultiHead Attention Refactor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "iI9kGQRJYhvD"
      },
      "source": [
        "Python `for` loops are slow, therefore it is better to refactor the MultiHeadAttention module to compute Q, K, V for all heads in batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "BsepMpkuYhvD"
      },
      "source": [
        "class MultiHeadAttention(Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        assert d_model%n_heads == 0\n",
        "        self.n_heads = n_heads\n",
        "        #d_qk, d_v = d_model//n_heads, d_model//n_heads\n",
        "        self.iq = nn.Linear(d_model, d_model)\n",
        "        self.ik = nn.Linear(d_model, d_model)\n",
        "        self.iv = nn.Linear(d_model, d_model)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        bs, seq_len, d = x.size()\n",
        "        # (bs,sl,d) -> (bs,sl,nh,dh) -> (bs,nh,sl,dh)\n",
        "        q = self.iq(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2)\n",
        "        k = self.ik(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2)\n",
        "        v = self.iv(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2)\n",
        "        att = F.softmax(q@k.transpose(-2,-1)/k.size(-1)**0.5, -1)\n",
        "        \n",
        "        out = att @ v # (bs, nh, sl, sl) x (bs, nh, sl, dh) -> (bs, nh, sl, dh)\n",
        "        out = out.transpose(1, 2).contiguous().view(bs, seq_len, d) # back to original shape\n",
        "\n",
        "        return self.out(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "IIvjWrL6YhvT",
        "outputId": "6ac2fe22-2b90-4ee1-9aac-83716aa22707"
      },
      "source": [
        "learn = Learner(dls, Model2(len(vocab)), loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n",
        "learn.fit_one_cycle(5, 1e-3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.751475</td>\n",
              "      <td>2.048160</td>\n",
              "      <td>0.422154</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.501326</td>\n",
              "      <td>1.906529</td>\n",
              "      <td>0.453054</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.547408</td>\n",
              "      <td>1.831737</td>\n",
              "      <td>0.449489</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.572775</td>\n",
              "      <td>1.673761</td>\n",
              "      <td>0.468029</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.563618</td>\n",
              "      <td>1.672115</td>\n",
              "      <td>0.468029</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "vtlyTr1MYhvX"
      },
      "source": [
        "Note that some speedup is observed even on such a tiny dataset and small model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQgJsZz8YhvY"
      },
      "source": [
        "### More signal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBEBry4-YhvZ"
      },
      "source": [
        "Similarly to the RNN case considered in the book, we can take the next step and create more signal for the model to learn from. To adapt to the modified objective we need to make couple of steps. First let's rearrange data to proper input-target pairs for the new task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkT4RsVoYhvd"
      },
      "source": [
        "#### Arranging data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMvldO7xYhve"
      },
      "source": [
        "Unlike RNN the tranformer is not a stateful model. This means it treats each sequence indepently and can only attend within fixed length context. This limitation was adressed by authors of [Transformer-XL paper](https://arxiv.org/abs/1901.02860) where adding a segment-level recurrence mechanism and a novel positional encoding scheme were proposed to enable capturing long-term dependencies. I will not go into details of TransformerXL architecture here. As will shell see stateless transformer can also learn a lot about the structure of our data.\n",
        "\n",
        "One thing to note in this case is that we don't need to maintain the structure of the data outside of the sequences, so we can shuffle the sequences randomly in the dataloader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ_Cyq9XYhve",
        "outputId": "768c7517-ed38-4f76-e934-1ba982c45819"
      },
      "source": [
        "sl = 16\n",
        "seqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))\n",
        "         for i in range(0,len(nums)-sl-1,sl))\n",
        "cut = int(len(seqs) * 0.8)\n",
        "dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:],\n",
        "                             bs=bs, drop_last=True, shuffle=True)\n",
        "xb, yb = dls.one_batch()\n",
        "xb.shape, yb.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 16]), torch.Size([64, 16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymd-JFQ3Yhvj",
        "outputId": "e67aa2d0-d15f-42c2-b70e-e61b8bf8e864"
      },
      "source": [
        "[L(vocab[o] for o in s) for s in seqs[0]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(#16) ['one','.','two','.','three','.','four','.','five','.'...],\n",
              " (#16) ['.','two','.','three','.','four','.','five','.','six'...]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKoBNYjnYhvn"
      },
      "source": [
        "#### Positional encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc-i0pfeYhvn"
      },
      "source": [
        "Before we did average pooling over seq_len dimention. Our model didn't care about the order of the tokens at all. But actually order of the tokens in a sentence matter a lot. In our case `one hundred two` and `two hundred one` are pretty different and `hundred one two` doesn't make sense.\n",
        "\n",
        "To encorporate positional information into the model authors of the transformer architecture proposed to use positional encodings in addition to regular token embeddings. Positional encodings may be learned, but it's also possible to use hardcoded encodings. For instance encodings may be composed of sin and cos.\n",
        "In this way each position in a sequence will get unique vector associated with it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io39VaVkYhvp"
      },
      "source": [
        "class PositionalEncoding(Module):\n",
        "    def __init__(self, d):\n",
        "        self.register_buffer('freq', 1/(10000 ** (torch.arange(0., d, 2.)/d)))\n",
        "        self.factor = d**0.5\n",
        "    def forward(self, x):\n",
        "        pos_enc = torch.cat([torch.sin(torch.outer(torch.arange(x.size(1)), self.freq)),\n",
        "                             torch.cos(torch.outer(torch.arange(x.size(1)), self.freq))],\n",
        "                            axis=-1)\n",
        "        return x/self.factor + pos_enc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RmOIvTIYhvs",
        "outputId": "14d59e83-6d90-46c5-9094-f9b072a73b8b"
      },
      "source": [
        "x = torch.zeros(1, 16, 64)\n",
        "encs = PositionalEncoding(64)(x)\n",
        "plt.matshow(encs.squeeze())\n",
        "plt.xlabel('Embedding size')\n",
        "plt.ylabel('Sequence length')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHjfpI46Yhvu"
      },
      "source": [
        "class TransformerEmbedding(Module):\n",
        "    def __init__(self, emb_sz, d_model):\n",
        "        self.emb = nn.Embedding(emb_sz, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model)\n",
        "    def forward(self, x):\n",
        "        return self.pos_enc(self.emb(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH3GZUb1Yhvy"
      },
      "source": [
        "class Model3(Module):\n",
        "    def __init__(self, vocab_sz, d_model=64, n_heads=4, d_ff=64*4):\n",
        "        self.emb = TransformerEmbedding(vocab_sz, d_model)\n",
        "        self.attn = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ff = FeedForward(d_model, d_ff)\n",
        "        self.out = nn.Linear(d_model, vocab_sz)\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        x = self.ff(self.attn(x))\n",
        "        return self.out(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPM--hHjYhv1",
        "outputId": "ef9684c7-b97d-4a47-d839-1a81561e9164"
      },
      "source": [
        "model = Model3(len(vocab))\n",
        "out = model(xb)\n",
        "out.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 16, 30])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKfZUXORYhv7"
      },
      "source": [
        "def loss_func(inp, targ):\n",
        "    return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kcwar5UYhwA",
        "outputId": "b8c489d8-3adb-4724-a7d7-fa6f7cd265c8"
      },
      "source": [
        "learn = Learner(dls, Model3(len(vocab)), loss_func=loss_func, metrics=accuracy)\n",
        "learn.fit_one_cycle(5, 1e-2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.855371</td>\n",
              "      <td>2.717834</td>\n",
              "      <td>0.157471</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.188235</td>\n",
              "      <td>1.624319</td>\n",
              "      <td>0.329753</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.153313</td>\n",
              "      <td>0.122275</td>\n",
              "      <td>0.971110</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.468753</td>\n",
              "      <td>0.064415</td>\n",
              "      <td>0.984212</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.212366</td>\n",
              "      <td>0.059186</td>\n",
              "      <td>0.985596</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chji-L_DYhwE"
      },
      "source": [
        "Wow! Thats a great accuracy! So the problem is solved and we only needed one attention layer and 2 layer deep feed-forward block? Don't you feel somewhat skeptical about this result?\n",
        "\n",
        "Well, you should be! Think about what we did here: the goal was to predict a target sequence, say `['.','two','.','three','.','four']` from an input `['one','.','two','.','three','.']`. These two sequences intersect on all positions except the first and the last one. So models needs to learn simply to copy input tokens starting from the second one to the outputs. In our case this will result in 15 correct predictions of total 16 positions, thats almost 94% accuracy. This makes the task very simple but not very useful to learn. To train proper autoregressive language model, as we did with RNNs, a concept of masking is to be introduced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQoUNmSHYhwE"
      },
      "source": [
        "#### Masking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAbHKWKnYhwF"
      },
      "source": [
        "So we want to allow the model for each token to attend only to itself and those prior to it. To acomplish this we can set all the values of attention matrix above the main diagonal to $-\\infty$. After softmax this values will effectively turn to 0 thus disabling attention to the \"future\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a96oz-jjYhwF"
      },
      "source": [
        "def get_subsequent_mask(x):\n",
        "    sz = x.size(1)\n",
        "    mask = (torch.triu(torch.ones(sz, sz, device=x.device)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USodFQDhYhwI",
        "outputId": "c03840e5-5429-4b9a-995a-9a75d109fb2c"
      },
      "source": [
        "inp = torch.randn(8, 10, 64)\n",
        "mask = get_subsequent_mask(inp)\n",
        "plt.matshow(mask);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5c6qnqyYhwL",
        "outputId": "9caca212-2e00-4040-f7aa-c5717726a620"
      },
      "source": [
        "q, k = torch.randn(8,10,32), torch.randn(8,10,32)\n",
        "att_ = F.softmax((q@k.permute(0,2,1)+mask), -1)\n",
        "plt.matshow(att_[0].detach());"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5l8M21yYhwN"
      },
      "source": [
        "We should also modify the attention layer to except mask:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irBKpSolYhwO"
      },
      "source": [
        "class MultiHeadAttention(Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        assert d_model%n_heads == 0\n",
        "        self.n_heads = n_heads\n",
        "        d_qk, d_v = d_model//n_heads, d_model//n_heads\n",
        "        self.iq = nn.Linear(d_model, d_model)\n",
        "        self.ik = nn.Linear(d_model, d_model)\n",
        "        self.iv = nn.Linear(d_model, d_model)\n",
        "        self.scale = d_qk**0.5\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def forward(self, x, mask=None):\n",
        "        bs, seq_len, d = x.size()\n",
        "        mask = ifnone(mask, 0)\n",
        "        k = self.ik(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2)\n",
        "        q = self.iq(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2)\n",
        "        v = self.iv(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2)\n",
        "        att = F.softmax(q@k.transpose(-2,-1)/self.scale + mask, -1)\n",
        "        \n",
        "        out = att @ v # (bs, nh, sl, sl) x (bs, nh, sl, dh) -> (bs, nh, sl, dh)\n",
        "        out = out.transpose(1, 2).contiguous().view(bs, seq_len, d) # back to original shape\n",
        "\n",
        "        return self.out(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pg0PGUcwYhwR"
      },
      "source": [
        "class Model4(Module):\n",
        "    def __init__(self, vocab_sz, d_model=64, n_heads=8, d_ff=64*4):\n",
        "        self.emb = TransformerEmbedding(vocab_sz, d_model)\n",
        "        self.attn = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ff = FeedForward(d_model, d_ff)\n",
        "        self.out = nn.Linear(d_model, vocab_sz)\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        mask = get_subsequent_mask(x)\n",
        "        x = self.ff(self.attn(x, mask))\n",
        "        return self.out(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5J_GgiAYhwU",
        "outputId": "27638523-e418-4fbc-fc6d-f7cdcd4458ca"
      },
      "source": [
        "learn = Learner(dls, Model4(len(vocab)), loss_func=loss_func, metrics=accuracy)\n",
        "learn.fit_one_cycle(5, 3e-3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.129320</td>\n",
              "      <td>2.853039</td>\n",
              "      <td>0.142497</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.817813</td>\n",
              "      <td>2.497936</td>\n",
              "      <td>0.241374</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.177413</td>\n",
              "      <td>1.822177</td>\n",
              "      <td>0.431234</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.771515</td>\n",
              "      <td>1.849530</td>\n",
              "      <td>0.394938</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.591972</td>\n",
              "      <td>1.817620</td>\n",
              "      <td>0.375895</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.510942</td>\n",
              "      <td>1.875521</td>\n",
              "      <td>0.334717</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.459658</td>\n",
              "      <td>1.882601</td>\n",
              "      <td>0.387126</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.425628</td>\n",
              "      <td>1.865636</td>\n",
              "      <td>0.407959</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.406187</td>\n",
              "      <td>1.890635</td>\n",
              "      <td>0.408610</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.396117</td>\n",
              "      <td>1.873219</td>\n",
              "      <td>0.409831</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccj-BCu7YhwX"
      },
      "source": [
        "Now we get somewhat lower accuracy, which is expected given that the task has become more difficult. Also training loss is significantly lower than validation loss, which means the model is overfitting. Let's see if the same approaches as was taken to RNNs can help."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h85PFqu7YhwX"
      },
      "source": [
        "#### Multilayer transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JON53-EyYhwY"
      },
      "source": [
        "To solve a more difficult task we ussualy need a deeper model. For convenience let's make a TransformerLayer which will combine self-attention and feed-forward blocks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWwi_PYfYhwY"
      },
      "source": [
        "class TransformerLayer(Module):\n",
        "    def __init__(self, d_model, n_heads=8, d_ff=None, causal=True):\n",
        "        d_ff = ifnone(d_ff, 4*d_model)\n",
        "        self.attn = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ff = FeedForward(d_model, d_ff)\n",
        "        self.causal = causal\n",
        "    def forward(self, x, mask=None):\n",
        "        if self.causal:\n",
        "            mask = get_subsequent_mask(x)\n",
        "        return self.ff(self.attn(x, mask))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sspERrjlYhwc"
      },
      "source": [
        "class Model5(Module):\n",
        "    def __init__(self, vocab_sz, d_model=64, n_layer=4, n_heads=8):\n",
        "        self.emb = TransformerEmbedding(vocab_sz, d_model)\n",
        "        self.tfmr = nn.Sequential(*[TransformerLayer(d_model, n_heads) for _ in range(n_layer)])\n",
        "        self.out = nn.Linear(d_model, vocab_sz)\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        x = self.tfmr(x)\n",
        "        return self.out(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_01PScwYhwe",
        "outputId": "0620e2dc-10b3-4acf-9637-45f2b716e1b2"
      },
      "source": [
        "learn = Learner(dls, Model5(len(vocab), n_layer=4), loss_func=loss_func, metrics=accuracy)\n",
        "learn.fit_one_cycle(5, 1e-2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.896508</td>\n",
              "      <td>2.770022</td>\n",
              "      <td>0.151774</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.791184</td>\n",
              "      <td>2.786991</td>\n",
              "      <td>0.151937</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.764605</td>\n",
              "      <td>2.786063</td>\n",
              "      <td>0.152018</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.753655</td>\n",
              "      <td>2.813671</td>\n",
              "      <td>0.151937</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.748509</td>\n",
              "      <td>2.806697</td>\n",
              "      <td>0.151611</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urkF3awxYhwh"
      },
      "source": [
        "That is not good 4 layer deep Transformer strugles to learn anything. But there are good news, this problem has been already resolved in the original transformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tX0T8zM2Yhwh"
      },
      "source": [
        "### Residual connections and Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT9zcxQoYhwi"
      },
      "source": [
        "If you are familiar with ResNets the proposed solution will not surprise you much. The idea is simple yet very effective. Instead of returning modified output $f(x)$ each transformer sublayer will return $x + f(x)$. Thishow to produce  allows the original input to propagate freely through the model. So the model learns not an entirely new representation of $x$ but how to modify $x$ to add some useful information to the original representation.\n",
        "\n",
        "As we modify layers to include the residual connections let's also add some regularization by inserting Dropout layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blCFVq59Yhwi"
      },
      "source": [
        "class TransformerEmbedding(Module):\n",
        "    def __init__(self, emb_sz, d_model, p=0.1):\n",
        "        self.emb = nn.Embedding(emb_sz, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model)\n",
        "        self.drop = nn.Dropout(p)\n",
        "    def forward(self, x):\n",
        "        return self.drop(self.pos_enc(self.emb(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfrOyTKjYhwl"
      },
      "source": [
        "Another modification is to add layer normalization which is intended to improve learning dynamics of the network by reparametrising data statistics and is generally used in transformer based architectures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC3ZuJ7lYhwm"
      },
      "source": [
        "class FeedForward(Module):\n",
        "    def __init__(self, d_model, d_ff, p=0.2):\n",
        "        self.lin1 = nn.Linear(d_model, d_ff)\n",
        "        self.lin2 = nn.Linear(d_ff, d_model)\n",
        "        self.act = nn.ReLU()\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(p)\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        out = self.act(self.lin1(x))\n",
        "        out = self.lin2(out)\n",
        "        return x + self.drop(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0t9MeALYhwp"
      },
      "source": [
        "class MultiHeadAttention(Module):\n",
        "    def __init__(self, d_model, n_heads, p=0.1):\n",
        "        assert d_model%n_heads == 0\n",
        "        self.n_heads = n_heads\n",
        "        d_qk, d_v = d_model//n_heads, d_model//n_heads\n",
        "        self.iq = nn.Linear(d_model, d_model)\n",
        "        self.ik = nn.Linear(d_model, d_model)\n",
        "        self.iv = nn.Linear(d_model, d_model)\n",
        "        self.scale = d_qk**0.5\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(p)\n",
        "    def forward(self, x, mask=None):\n",
        "        bs, seq_len, d = x.size()\n",
        "        mask = ifnone(mask, 0)\n",
        "        x = self.norm(x)\n",
        "        k = self.ik(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2)\n",
        "        q = self.iq(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2)\n",
        "        v = self.iv(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2)\n",
        "        att = F.softmax(q@k.transpose(-2,-1)/self.scale + mask, -1)\n",
        "        \n",
        "        out = att @ v # (bs, nh, sl, sl) x (bs, nh, sl, dh) -> (bs, nh, sl, dh)\n",
        "        out = out.transpose(1, 2).contiguous().view(bs, seq_len, d) # back to original shape\n",
        "\n",
        "        return x + self.drop(self.out(out))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3FkBTm9Yhwu"
      },
      "source": [
        "class TransformerLayer(Module):\n",
        "    def __init__(self, d_model, n_heads=8, d_ff=None, causal=True,\n",
        "                 p_att=0.1, p_ff=0.1):\n",
        "        d_ff = ifnone(d_ff, 4*d_model)\n",
        "        self.attn = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ff = FeedForward(d_model, d_ff, p=p_ff)\n",
        "        self.causal = causal\n",
        "    def forward(self, x, mask=None):\n",
        "        if self.causal:\n",
        "            mask = get_subsequent_mask(x)\n",
        "        return self.ff(self.attn(x, mask))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erFIZd3eYhww"
      },
      "source": [
        "class Model6(Module):\n",
        "    def __init__(self, vocab_sz, d_model=64, n_layer=4, n_heads=8, \n",
        "                 p_emb=0.1, p_att=0.1, p_ff=0.2):\n",
        "        self.emb = TransformerEmbedding(vocab_sz, d_model, p=p_emb)\n",
        "        self.tfmr = nn.Sequential(*[TransformerLayer(d_model, n_heads,\n",
        "                                                     p_att=p_att, p_ff=p_ff)\n",
        "                                    for _ in range(n_layer)])\n",
        "        self.out = nn.Linear(d_model, vocab_sz)\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        x = self.tfmr(x)\n",
        "        return self.out(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYIO7Ax9Yhwz",
        "outputId": "891dc3da-2464-47a1-97ed-ed50184b2700"
      },
      "source": [
        "learn = Learner(dls, Model6(len(vocab), n_layer=2), loss_func=loss_func, metrics=accuracy)\n",
        "learn.fit_one_cycle(10, 1e-2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.813592</td>\n",
              "      <td>2.619886</td>\n",
              "      <td>0.112874</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.143356</td>\n",
              "      <td>1.601458</td>\n",
              "      <td>0.451986</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.718504</td>\n",
              "      <td>1.667646</td>\n",
              "      <td>0.456217</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.364230</td>\n",
              "      <td>0.986376</td>\n",
              "      <td>0.673991</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.021568</td>\n",
              "      <td>0.807140</td>\n",
              "      <td>0.765462</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.793330</td>\n",
              "      <td>0.725030</td>\n",
              "      <td>0.775553</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.672081</td>\n",
              "      <td>0.741638</td>\n",
              "      <td>0.766683</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.610938</td>\n",
              "      <td>0.787549</td>\n",
              "      <td>0.743001</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.579836</td>\n",
              "      <td>0.747094</td>\n",
              "      <td>0.755371</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.562835</td>\n",
              "      <td>0.758436</td>\n",
              "      <td>0.744873</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV7zOfG9Yhw2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}